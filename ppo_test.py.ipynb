{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ppo_test.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNaA+jw8HeubEQt10jQofbz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"r06APHeUK1_l"},"source":["#drive-data setup\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","%cd '/content/gdrive/My Drive/sem7/cs6886:sysdl/rl4dlc/sun/ppo_test/'\n","!ls\n","current_loc=!pwd\n","print(current_loc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nFw2SwVK_Xi"},"source":["#package imports\n","import torch\n","import torch.nn as nn\n","print(torch.__version__)\n","import gym\n","import copy\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iu4ZZmqML-LL"},"source":["#rl-agent\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class PolicywithValue(nn.Module):\n","    def __init__(self,obs_space,act_space):\n","        super(PolicywithValue,self).__init__()\n","        self.obs_dim,self.act_dim = obs_space,act_space\n","        policy_h1,policy_h2 = 20,20  #policy_hidden_nodes\n","        value_h1,value_h2 = 20,20  #value_hidden_nodes\n","        self.policy_l1 = nn.Linear(self.obs_dim,policy_h1)  #policy_network: policy_layer1\n","        self.policy_l2 = nn.Linear(policy_h1,policy_h2)\n","        self.policy_l3 = nn.Linear(policy_h2,self.act_dim) \n","        self.value_l1 = nn.Linear(self.obs_dim,value_h1)  #value_network: value_layer1\n","        self.value_l2 = nn.Linear(value_h1,value_h2)\n","        self.value_l3 = nn.Linear(value_h2,1)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self,x,activation='tanh',temp=1):\n","        if activation=='tanh':\n","            temp=0.1\n","            self.actv=self.tanh\n","        elif activation=='relu':\n","            self.actv=self.relu\n","            temp=1\n","        out_p1=self.actv(self.policy_l1(x))\n","        out_p2=self.actv(self.policy_l2(out_p1))\n","        out_p3=self.actv(self.policy_l3(out_p2))\n","        self.act_probs=self.softmax(torch.div(out_p3,temp))  #,dim=-1)\n","        out_v1=self.actv(self.value_l1(x))\n","        out_v2=self.actv(self.value_l2(out_v1))\n","        self.v_preds=self.value_l3(out_v2) \n","        self.act_deterministic=torch.argmax(self.act_probs,axis=-1)\n","        self.act_stochastic=torch.reshape(torch.multinomial(self.act_probs,num_samples=1),shape=[-1])  #torch.log(self.act_probs)! \n","\n","class PPOAgent(nn.Module):\n","    def __init__(self, policy, old_policy, horizon, learning_rate, epochs,batch_size, gamma, lmbd, clip_value, value_coeff, entropy_coeff, update_freq, memory_size,scheduler=False):\n","        super(PPOAgent,self).__init__()\n","        self.policy = policy\n","        self.old_policy = old_policy\n","        self.horizon = horizon\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(),lr=learning_rate,eps=1e-5)\n","        self.scheduler = scheduler\n","        if scheduler:\n","            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.epochs,gamma=0.999)  \n","        self.criterion = nn.MSELoss()\n","        self.gamma = gamma\n","        self.lmbd = lmbd\n","        self.clip_value = clip_value\n","        self.value_coeff = value_coeff\n","        self.entropy_coeff = entropy_coeff\n","        self.update_freq = update_freq\n","        self.memory_size = memory_size\n","        self.list_observations = []  #memory\n","        self.list_actions = []\n","        self.list_v_preds = []\n","        self.list_rewards = []\n","        self.count=0        \n","\n","    def _to_one_hot(self, y, num_classes):\n","        scatter_dim = len(y.size())\n","        y_tensor = y.view(*y.size(), -1).type(torch.int64)\n","        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype).to(device)\n","        return zeros.scatter(scatter_dim, y_tensor, 1)\n","\n","    def forward(self, observation, stochastic=True):\n","        self.policy(torch.Tensor(observation).to(device).type(torch.float32))\n","        act = policy.act_stochastic if stochastic else policy.act_deterministic\n","        act = act.item()\n","        v_pred = self.policy.v_preds\n","        v_pred = v_pred.item()\n","        if len(self.list_observations)>=self.memory_size:\n","            self.list_observations=self.list_observations[1:]\n","            self.list_actions=self.list_actions[1:]\n","            self.list_v_preds=self.list_v_preds[1:]\n","        self.list_observations.append(observation)\n","        self.list_actions.append(act)\n","        self.list_v_preds.append(v_pred)\n","        return act, v_pred\n","\n","    def update(self, reward, terminal):\n","        if len(self.list_rewards)>=self.memory_size:\n","            self.list_rewards=self.list_rewards[1:]\n","        self.list_rewards.append(reward) \n","        if terminal == False:\n","            return\n","        else:\n","            self.count+=1\n","            if self.count%self.update_freq==0:\n","                print('db: ',self.count,len(self.list_rewards),len(self.list_observations),len(self.list_actions),len(self.list_v_preds))\n","                assert len(self.list_rewards)==len(self.list_observations)==len(self.list_actions)==len(self.list_v_preds)\n","                self.list_v_preds_next = self.list_v_preds[1:] + [0]  #v_preds_next from v_preds\n","                self.list_gaes = self._get_gaes(self.list_rewards, self.list_v_preds, self.list_v_preds_next)  #generalized advantage estimations\n","                observations = torch.reshape(torch.Tensor(self.list_observations), shape=(-1,self.policy.obs_dim)).to(device).type(torch.float32) \n","                actions = torch.Tensor(self.list_actions).type(torch.int32).to(device)\n","                rewards = torch.Tensor(self.list_rewards).type(torch.float32).to(device)\n","                v_preds_next = torch.Tensor(self.list_v_preds_next).type(torch.float32).to(device)\n","                gaes = torch.Tensor(self.list_gaes).type(torch.float32).to(device)\n","                gaes = (gaes - gaes.mean()) / gaes.std()\n","                input_samples = [observations, actions, rewards, v_preds_next, gaes]\n","                self._update_old_policy()  \n","                if self.horizon != -1:  #sample horizon\n","                    horizon_indices = torch.Tensor(np.random.randint(low=0, high=observations.shape[0], size=self.horizon)).dtype(torch.int64).to(device)\n","                    horizon_samples = [input_sample[horizon_indices] for input_sample in input_samples]\n","                for epoch in range(self.epochs):\n","                    if self.horizon != -1:\n","                        batch_indices = torch.Tensor(np.random.randint(low=0, high=self.horizon, size=self.batch_size)).type(torch.int64).to(device)\n","                        batch_samples = [input_sample[batch_indices] for input_sample in horizon_samples]\n","                    else:\n","                        batch_indices = torch.Tensor(np.random.randint(low=0, high=observations.shape[0], size=self.batch_size)).type(torch.int64).to(device)\n","                        batch_samples = [input_sample[batch_indices] for input_sample in input_samples]\n","                    self.learn(observations=batch_samples[0], actions=batch_samples[1], rewards=batch_samples[2], v_preds_next=batch_samples[3], gaes=batch_samples[4])\n","                self.list_observations = []\n","                self.list_actions = []\n","                self.list_v_preds = []\n","                self.list_rewards = []\n","\n","    def learn(self, observations, actions, rewards, v_preds_next, gaes, stochastic=True):\n","        self.policy(observations)\n","        self.old_policy(observations)\n","        act_probs = self.policy.act_probs\n","        act_probs_old = self.old_policy.act_probs\n","#        print('act:1',act_probs)\n","        act_probs = act_probs * self._to_one_hot(actions,num_classes=act_probs.shape[-1])  #tf.one_hot(indices=self.actions, depth=act_probs.shape[-1])#\n","#        print('act:2',act_probs)\n","        act_probs = torch.sum(act_probs, axis=1)\n","        act_probs_old = act_probs_old * self._to_one_hot(actions,num_classes=act_probs_old.shape[-1])  \n","        act_probs_old = torch.sum(act_probs_old, axis=1)\n","        ratios = torch.exp(torch.log(act_probs)-torch.log(act_probs_old))  #clipped surrogate objective\n","#        print('ratios:',ratios)\n","        clipped_ratios = torch.clamp(ratios, 1-self.clip_value, 1+self.clip_value)\n","#        print('clipped_ratios: ',clipped_ratios)\n","        loss_clip = torch.min(torch.mul(gaes, ratios), torch.mul(gaes, clipped_ratios))\n","#        print('loss_clip: ',loss_clip)\n","        loss_clip = torch.mean(loss_clip)\n","#        print('loss_clip: ',loss_clip)\n","        v_preds = self.policy.v_preds\n","#        print('v_preds: ',v_preds)\n","#        print('inp1: ',torch.unsqueeze(rewards + self.gamma * v_preds_next,axis=-1))\n","#        print('inp2: ',v_preds)\n","        loss_v = self.criterion(torch.unsqueeze(rewards + self.gamma * v_preds_next,axis=-1), v_preds)\n","#        print('sum:',self.policy.act_probs * torch.log(torch.clamp(self.policy.act_probs, 1e-10, 1.0)))\n","        entropy = -torch.sum(self.policy.act_probs * torch.log(torch.clamp(self.policy.act_probs, 1e-10, 1.0)), axis=1)\n","#        print('entropy: ',entropy)\n","        entropy = torch.mean(entropy, axis=0)\n","#        print('entropy2: ',entropy)\n","        loss = loss_clip - self.value_coeff * loss_v + self.entropy_coeff * entropy\n","#        print('loss1: ',loss)\n","        loss = -loss\n","#        print('loss2: ',loss)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        if self.scheduler:\n","            self.scheduler.step()\n","\n","    def _get_gaes(self, rewards, v_preds, v_preds_next):  #generalized advantage estimate\n","        deltas = [r + self.gamma * v_next - v for r, v_next, v in zip(rewards, v_preds_next, v_preds)]\n","        gaes = copy.deepcopy(deltas)\n","        for t in reversed(range(len(gaes) - 1)):\n","            gaes[t] = gaes[t] + self.gamma * self.lmbd * gaes[t+1]\n","        return gaes\n","\n","    def _update_old_policy(self):  #update old policy with policy\n","        for old_param,param in zip(self.old_policy.parameters(),self.policy.parameters()):\n","            old_param.data=param.data.clone().detach()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGlesZQKIt9Y"},"source":["#main\n","render = False\n","env = gym.make('CartPole-v1')\n","print(len(env.observation_space.shape))  #box\n","print(len(env.action_space.shape))  #discrete\n","if len(env.action_space.shape) >= 1:\n","    obs_space = env.observation_space.shape[-1]\n","    act_space = env.action_space.shape[-1]\n","else:\n","    obs_space = env.observation_space.shape[-1]\n","    act_space = env.action_space.n\n","policy = PolicywithValue(obs_space, act_space).to(device)\n","old_policy = PolicywithValue(obs_space, act_space).to(device)\n","agent = PPOAgent(policy, old_policy, \n","                 horizon=-1, \n","                 learning_rate=0.02,  #1e-4, \n","                 epochs=4, \n","                 batch_size=128, \n","                 gamma=0.99,  #0.95,0.99\n","                 lmbd=0.95,  #1.0,0.95\n","                 clip_value=0.2, \n","                 value_coeff=1.0, \n","                 entropy_coeff=0.01,\n","                 update_freq=1,\n","                 memory_size=400).to(device)\n","count=0\n","rewards=[]\n","for e in range(2000):\n","    avg_reward=0\n","    observation = env.reset()  #initialize OpenAI Gym environment\n","    for t in range(500):\n","        if render:\n","            env.render()\n","        # Query the agent for its action decision\n","        action, value  = agent(observation)\n","        #print(action, value)\n","        # Execute the decision and retrieve the current performance\n","        observation, reward, done, info = env.step(action)\n","        avg_reward+=reward\n","        # Modify reward so that negative reward is given when it finishes too early\n","        # Pass feedback about performance (and termination) to the agent\n","        agent.update(reward=reward, terminal=done)\n","        if done:\n","            print(\"Episode {} finished after {} timesteps with reward {}\".format(e+1, t+1, avg_reward))\n","            rewards.append(avg_reward)\n","            if avg_reward>195.0:\n","                count+=1\n","            else:\n","                count=0\n","            if count==100:\n","                print('done!')\n","#                exit(0)\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYP6FtUb2HVk"},"source":["from matplotlib import pyplot as plt\n","plt.plot(rewards)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4thSg0pSrVb"},"source":["env.observation_space.shape[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw82n03T0L_c"},"source":["policy.act_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC2Ce8m1U1Zx"},"source":["policy.act_probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lwwEtdDMmFe"},"source":["env.action_space.sample().shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRVUD5FlXWXw"},"source":["import torch\n","import gym\n","import torch.optim as optim\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class MlpPolicy(nn.Module):\n","    def __init__(self, action_size, input_size=4):\n","        super(MlpPolicy, self).__init__()\n","        self.action_size = action_size\n","        self.input_size = input_size\n","        self.fc1 = nn.Linear(self.input_size, 24)\n","        self.fc2 = nn.Linear(24, 24)\n","        self.fc3_pi = nn.Linear(24, self.action_size)\n","        self.fc3_v = nn.Linear(24, 1)\n","        self.tanh = nn.Tanh()\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def pi(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3_pi(x)\n","        return self.softmax(x)\n","\n","    def v(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3_v(x)\n","        return x\n","\n","class AgentConfig:\n","    gamma = 0.99  #Learning parameters\n","    plot_every = 10\n","    update_freq = 1\n","    k_epoch = 3\n","    learning_rate = 0.02\n","    lmbda = 0.95\n","    eps_clip = 0.2\n","    v_coef = 1\n","    entropy_coef = 0.01\n","\n","    # Memory\n","    memory_size = 400\n","\n","    train_cartpole = True\n","\n","class Agent(AgentConfig):\n","    def __init__(self):\n","        self.env = gym.make('CartPole-v0')\n","        self.action_size = self.env.action_space.n  # 2 for cartpole\n","        if self.train_cartpole:\n","            self.policy_network = MlpPolicy(action_size=self.action_size).to(device)\n","        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.learning_rate)    \n","#        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.k_epoch,gamma=0.999)\n","        self.memory = {\n","            'state': [], 'action': [], 'reward': [], 'next_state': [], 'action_prob': [], 'terminal': [], 'count': 0,\n","            'advantage': [], 'td_target': torch.FloatTensor([])\n","        }\n","        self.loss = 0\n","        self.criterion = nn.MSELoss()\n","\n","    def new_random_game(self):\n","        self.env.reset()\n","        action = self.env.action_space.sample()\n","        screen, reward, terminal, info = self.env.step(action)\n","        return screen, reward, action, terminal\n","\n","    def train(self):\n","        episode = 0\n","        step = 0\n","        reward_history = []\n","        avg_reward = []\n","        solved = False\n","\n","        # A new episode\n","        while not solved:\n","            start_step = step\n","            episode += 1\n","            episode_length = 0\n","\n","            # Get initial state\n","            state, reward, action, terminal = self.new_random_game()\n","            current_state = state\n","            total_episode_reward = 1\n","\n","            # A step in an episode\n","            while not solved:\n","                step += 1\n","                episode_length += 1\n","\n","                # Choose action\n","                prob_a = self.policy_network.pi(torch.FloatTensor(current_state).to(device))\n","                # print(prob_a)\n","                action = torch.distributions.Categorical(prob_a).sample().item()\n","\n","                # Act\n","                state, reward, terminal, _ = self.env.step(action)\n","                new_state = state\n","\n","                reward = -1 if terminal else reward\n","\n","                self.add_memory(current_state, action, reward/10.0, new_state, terminal, prob_a[action].item())\n","\n","                current_state = new_state\n","                total_episode_reward += reward\n","\n","                if terminal:\n","                    episode_length = step - start_step\n","                    reward_history.append(total_episode_reward)\n","                    avg_reward.append(sum(reward_history[-10:])/10.0)\n","\n","                    self.finish_path(episode_length)\n","\n","                    if len(reward_history) > 100 and sum(reward_history[-100:-1]) / 100 >= 195:\n","                        solved = True\n","                        exit(0)\n","\n","                    print('episode: %.2f, total step: %.2f, last_episode length: %.2f, last_episode_reward: %.2f, '\n","                          'loss: %.4f' % (episode, step, episode_length, total_episode_reward, self.loss))#,self.scheduler.get_lr()[0]))\n","\n","                    self.env.reset()\n","\n","                    break\n","\n","            if episode % self.update_freq == 0:\n","                for _ in range(self.k_epoch):\n","                    self.update_network()\n","\n","            if episode % self.plot_every == 0:\n","                plot_graph(reward_history, avg_reward)\n","\n","        self.env.close()\n","\n","    def update_network(self):\n","        # get ratio\n","        pi = self.policy_network.pi(torch.FloatTensor(self.memory['state']).to(device))\n","        new_probs_a = torch.gather(pi, 1, torch.tensor(self.memory['action']).to(device))\n","        old_probs_a = torch.FloatTensor(self.memory['action_prob']).to(device)\n","        ratio = torch.exp(torch.log(new_probs_a) - torch.log(old_probs_a))\n","\n","        # surrogate loss\n","        surr1 = ratio * torch.FloatTensor(self.memory['advantage']).to(device)\n","        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * torch.FloatTensor(self.memory['advantage']).to(device)\n","        pred_v = self.policy_network.v(torch.FloatTensor(self.memory['state']).to(device))\n","        v_loss = 0.5 * (pred_v - self.memory['td_target']).pow(2)  # Huber loss\n","        entropy = torch.distributions.Categorical(pi).entropy()\n","        entropy = torch.tensor([[e] for e in entropy]).to(device)\n","        self.loss = (-torch.min(surr1, surr2) + self.v_coef * v_loss.to(device) - self.entropy_coef * entropy).mean()\n","\n","        self.optimizer.zero_grad()\n","        self.loss.backward()\n","        self.optimizer.step()\n","#        self.scheduler.step()\n","\n","    def add_memory(self, s, a, r, next_s, t, prob):\n","        if self.memory['count'] < self.memory_size:\n","            self.memory['count'] += 1\n","        else:\n","            self.memory['state'] = self.memory['state'][1:]\n","            self.memory['action'] = self.memory['action'][1:]\n","            self.memory['reward'] = self.memory['reward'][1:]\n","            self.memory['next_state'] = self.memory['next_state'][1:]\n","            self.memory['terminal'] = self.memory['terminal'][1:]\n","            self.memory['action_prob'] = self.memory['action_prob'][1:]\n","            self.memory['advantage'] = self.memory['advantage'][1:]\n","            self.memory['td_target'] = self.memory['td_target'][1:]\n","\n","        self.memory['state'].append(s)\n","        self.memory['action'].append([a])\n","        self.memory['reward'].append([r])\n","        self.memory['next_state'].append(next_s)\n","        self.memory['terminal'].append([1 - t])\n","        self.memory['action_prob'].append(prob)\n","\n","    def finish_path(self, length):\n","        state = self.memory['state'][-length:]\n","        reward = self.memory['reward'][-length:]\n","        next_state = self.memory['next_state'][-length:]\n","        terminal = self.memory['terminal'][-length:]\n","\n","        td_target = torch.FloatTensor(reward).to(device) + \\\n","                    self.gamma * self.policy_network.v(torch.FloatTensor(next_state).to(device)) * torch.FloatTensor(terminal).to(device)\n","        delta = td_target - self.policy_network.v(torch.FloatTensor(state).to(device))\n","        delta = delta.detach().cpu().numpy()\n","\n","        # get advantage\n","        advantages = []\n","        adv = 0.0\n","        for d in delta[::-1]:\n","            adv = self.gamma * self.lmbda * adv + d[0]\n","            advantages.append([adv])\n","        advantages.reverse()\n","\n","        if self.memory['td_target'].shape == torch.Size([1, 0]):\n","            self.memory['td_target'] = td_target.data\n","        else:\n","            self.memory['td_target'] = torch.cat((self.memory['td_target'].to(device), td_target.data.to(device)), dim=0)\n","        self.memory['advantage'] += advantages\n","\n","\n","def plot_graph(reward_history, avg_reward):\n","    df = pd.DataFrame({'x': range(len(reward_history)), 'Reward': reward_history, 'Average': avg_reward})\n","    plt.style.use('seaborn-darkgrid')\n","    palette = plt.get_cmap('Set1')\n","\n","    plt.plot(df['x'], df['Reward'], marker='', color=palette(1), linewidth=0.8, alpha=0.9, label='Reward')\n","    # plt.plot(df['x'], df['Average'], marker='', color='tomato', linewidth=1, alpha=0.9, label='Average')\n","\n","    # plt.legend(loc='upper left')\n","    plt.title(\"CartPole\", fontsize=14)\n","    plt.xlabel(\"episode\", fontsize=12)\n","    plt.ylabel(\"score\", fontsize=12)\n","\n","    plt.savefig('score.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HA6p10VXXnPr"},"source":["def main():\n","    agent = Agent()\n","    agent.train()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKRw8hmcYCV_"},"source":["print(device)"],"execution_count":null,"outputs":[]}]}