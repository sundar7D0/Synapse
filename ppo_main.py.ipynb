{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ppo_main.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaaiaB5NNmIyozGUig/wZl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5_WK3DbGnDRJ"},"source":["#drive-data setup\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","%cd '/content/gdrive/My Drive/sem7/cs6886:sysdl/rl4dlc/sun/expts/'\n","!ls\n","current_loc=!pwd\n","print(current_loc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SOIOJ8OnYH0"},"source":["#package imports\n","import torch\n","import torch.nn as nn\n","print(torch.__version__)\n","import gym\n","import copy\n","import random\n","import math\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hS3AbVgQnZ3u"},"source":["#rl-agent\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class PolicywithValue(nn.Module):\n","    def __init__(self,obs_space,act_space):\n","        super(PolicywithValue,self).__init__()\n","        assert len(obs_space)==1\n","        self.obs_dim,self.act_space = obs_space[-1],act_space\n","        policy_c_ = [self.obs_dim,20,20]  #policy_common_nodes\n","        policy_uc_ = []  #policy_un-common_nodes\n","        for a in range(len(act_space)):\n","            policy_uc_.append([policy_c_[-1]]+[act_space[a]]*3)\n","        value_ = [self.obs_dim,20,20,20,1]  #value_nodes\n","        self.policy_c,self.policy_uc=nn.ModuleList([]),nn.ModuleList([])\n","        for pc in range(len(policy_c_)-1):  #policy_network_layers(common)\n","            self.policy_c.append(nn.Linear(policy_c_[pc],policy_c_[pc+1]))\n","        for a in range(len(act_space)):  #policy_network_layers(un-common)\n","            self.policy_uc.append(nn.ModuleList([]))\n","            for puc in range(len(policy_uc_[a])-1):\n","                self.policy_uc[a].append(nn.Linear(policy_uc_[a][puc],policy_uc_[a][puc+1]))\n","        self.value = nn.ModuleList([])\n","        for v in range(len(value_)-1):  #value_network_layers\n","            self.value.append(nn.Linear(value_[v],value_[v+1]))\n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self,x,activation='tanh',temp=1):\n","        if activation=='tanh':\n","            self.actv=self.tanh\n","            temp=0.1\n","        elif activation=='relu':\n","            self.actv=self.relu\n","            temp=1\n","        out_pc,out_v=x.clone(),x.clone()\n","        for pc in range(len(self.policy_c)):\n","            out_pc=self.actv(self.policy_c[pc](out_pc))\n","        self.act_probs,self.act_deterministic,self.act_stochastic=[],[],[]\n","        for a in range(len(self.policy_uc)):\n","            out_puc=out_pc.clone()\n","            for puc in range(len(self.policy_uc[a])):\n","                out_puc=self.actv(self.policy_uc[a][puc](out_puc))\n","            act_probs=torch.softmax(torch.div(out_puc,temp),dim=-1)\n","            self.act_probs.append(act_probs)\n","            self.act_deterministic.append(torch.argmax(act_probs,axis=-1))\n","            self.act_stochastic.append(torch.reshape(torch.multinomial(act_probs,num_samples=1),shape=[-1]))  #!torch.log(self.act_probs)\n","        for v in range(len(self.value)-1):\n","            out_v=self.actv(self.value[v](out_v))\n","        self.v_preds=self.value[-1](out_v)  #linear\n","\n","class PPOAgent(nn.Module):\n","    def __init__(self, policy, old_policy, horizon, learning_rate, epochs,batch_size, gamma, lmbd, clip_value, value_coeff, entropy_coeff, update_freq, memory_size, schedule=False):\n","        super(PPOAgent,self).__init__()\n","        self.policy = policy\n","        self.old_policy = old_policy\n","        self.horizon = horizon  #hyper_parameters\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(),lr=learning_rate,eps=1e-5)\n","        self.schedule = schedule\n","        if schedule:\n","            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=self.epochs,gamma=0.999)  \n","        self.criterion = nn.MSELoss()\n","        self.gamma = gamma\n","        self.lmbd = lmbd\n","        self.clip_value = clip_value\n","        self.value_coeff = value_coeff\n","        self.entropy_coeff = entropy_coeff\n","        self.update_freq = update_freq\n","        self.memory_size = memory_size\n","        self.list_observations = []  #memory_elements\n","        self.list_actions = []\n","        self.list_v_preds = []\n","        self.list_rewards = []\n","        self.count=0\n","\n","    def _to_one_hot(self, y, num_classes):\n","        scatter_dim = len(y.size())\n","        y_tensor = y.view(*y.size(), -1).type(torch.int64)\n","        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype).to(device)\n","        return zeros.scatter(scatter_dim, y_tensor, 1)\n","\n","    def forward(self, observation, stochastic=True):\n","        self.policy(torch.Tensor(observation).to(device).type(torch.float32))\n","        actions = self.policy.act_stochastic if stochastic else self.policy.act_deterministic\n","        for a in range(len(actions)):\n","            actions[a] = actions[a].item()\n","        v_pred = self.policy.v_preds\n","        for v in range(len(v_pred)):\n","            v_pred[v] = v_pred[v].item()\n","        if len(self.list_observations)>=self.memory_size:\n","            self.list_observations=self.list_observations[1:]\n","            self.list_actions=self.list_actions[1:]\n","            self.list_v_preds=self.list_v_preds[1:]\n","        self.list_observations.append(observation)\n","        self.list_actions.append(actions)\n","        self.list_v_preds.append(v_pred)\n","        return actions, v_pred\n","\n","    def update(self, reward, terminal):\n","        if len(self.list_rewards)>=self.memory_size:\n","            self.list_rewards=self.list_rewards[1:]\n","        self.list_rewards.append(reward) \n","        if terminal == False:\n","            return\n","        else:\n","            self.count+=1\n","            if self.count%self.update_freq==0:\n","                assert len(self.list_rewards)==len(self.list_observations)==len(self.list_actions)==len(self.list_v_preds) and len(self.list_v_preds)<=self.memory_size\n","                self.list_v_preds_next = self.list_v_preds[1:] + [0]  #v_preds_next from v_preds\n","                self.list_gaes = self._get_gaes(self.list_rewards, self.list_v_preds, self.list_v_preds_next)  #generalized advantage estimations\n","                observations = torch.reshape(torch.Tensor(self.list_observations), shape=(-1,self.policy.obs_dim)).to(device).type(torch.float32) \n","                actions = torch.Tensor(self.list_actions).type(torch.int32).to(device)\n","                rewards = torch.Tensor(self.list_rewards).type(torch.float32).to(device)\n","                v_preds_next = torch.Tensor(self.list_v_preds_next).type(torch.float32).to(device)\n","                gaes = torch.Tensor(self.list_gaes).type(torch.float32).to(device)\n","                gaes = (gaes - gaes.mean()) / gaes.std()\n","                input_samples = [observations, actions, rewards, v_preds_next, gaes]\n","                self._update_old_policy()  #update old_policy with policy params\n","                if self.horizon != -1:  #sample horizon\n","                    horizon_indices = torch.Tensor(np.random.randint(low=0, high=observations.shape[0], size=self.horizon)).dtype(torch.int64).to(device)\n","                    horizon_samples = [input_sample[horizon_indices] for input_sample in input_samples]\n","                for epoch in range(self.epochs):\n","                    if self.horizon != -1:\n","                        batch_indices = torch.Tensor(np.random.randint(low=0, high=self.horizon, size=self.batch_size)).type(torch.int64).to(device)\n","                        batch_samples = [input_sample[batch_indices] for input_sample in horizon_samples]\n","                    else:\n","                        batch_indices = torch.Tensor(np.random.randint(low=0, high=observations.shape[0], size=self.batch_size)).type(torch.int64).to(device)\n","                        batch_samples = [input_sample[batch_indices] for input_sample in input_samples]\n","                    self.learn(observations=batch_samples[0], actions=batch_samples[1], rewards=batch_samples[2], v_preds_next=batch_samples[3], gaes=batch_samples[4])\n","                self.list_observations = []\n","                self.list_actions = []\n","                self.list_v_preds = []\n","                self.list_rewards = []\n","\n","    def learn(self, observations, actions, rewards, v_preds_next, gaes, stochastic=True):\n","        self.policy(observations)\n","        self.old_policy(observations)\n","        loss=[]\n","        for i in range(len(self.policy.act_space)):\n","            act_probs = self.policy.act_probs[i]\n","            act_probs_old = self.old_policy.act_probs[i]\n","            act_probs = act_probs * self._to_one_hot(actions[:,i],num_classes=act_probs.shape[-1])\n","            act_probs = torch.sum(act_probs, axis=-1)\n","            act_probs_old = act_probs_old * self._to_one_hot(actions[:,i],num_classes=act_probs_old.shape[-1])  \n","            act_probs_old = torch.sum(act_probs_old, axis=-1)\n","            ratios = torch.exp(torch.log(act_probs)-torch.log(act_probs_old))  \n","            clipped_ratios = torch.clamp(ratios, 1-self.clip_value, 1+self.clip_value)\n","            loss_clip = torch.min(torch.mul(gaes, ratios), torch.mul(gaes, clipped_ratios))  #clipped surrogate objective\n","            loss_clip = torch.mean(loss_clip)\n","            entropy = -torch.sum(self.policy.act_probs[i] * torch.log(torch.clamp(self.policy.act_probs[i], 1e-10, 1.0)), axis=1)\n","            entropy = torch.mean(entropy, axis=0)\n","            loss.append(loss_clip + self.entropy_coeff * entropy)\n","        v_preds = self.policy.v_preds\n","        loss_v = self.criterion(torch.unsqueeze(rewards + self.gamma * v_preds_next,axis=-1), v_preds)\n","        loss = sum(loss) - self.value_coeff * loss_v\n","        loss = -loss\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        if self.schedule:\n","            self.scheduler.step()\n","\n","    def _get_gaes(self, rewards, v_preds, v_preds_next):  #generalized advantage estimate\n","        deltas = [r + self.gamma * v_next - v for r, v_next, v in zip(rewards, v_preds_next, v_preds)]\n","        gaes = torch.Tensor(deltas).clone()\n","        for t in reversed(range(len(gaes) - 1)):\n","            gaes[t] = gaes[t] + self.gamma * self.lmbd * gaes[t+1]\n","        return gaes\n","\n","    def _update_old_policy(self):  #update old policy with policy\n","        for old_param,param in zip(self.old_policy.parameters(),self.policy.parameters()):\n","            old_param.data=param.data.clone().detach()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEsbtY51NXzf"},"source":["#main\n","import env as e\n","env = e.envi()  \n","policy = PolicywithValue(env.state_shape,env.action_shape).to(device)\n","old_policy = PolicywithValue(env.state_shape,env.action_shape).to(device)\n","agent = PPOAgent(policy, old_policy, \n","                 horizon=-1, \n","                 learning_rate=1e-3,  #0.02,1e-4, \n","                 epochs=3, \n","                 batch_size=64, \n","                 gamma=0.9,  #0.95,0.99\n","                 lmbd=0.99,  #1.0,0.95\n","                 clip_value=0.3, \n","                 value_coeff=1.0, \n","                 entropy_coeff=0.1,\n","                 update_freq=1,\n","                 memory_size=1000).to(device)\n","\n","for e in range(10):  #128,30\n","    avg_reward=0\n","    observation = env.reset() \n","    for t in range(500):  \n","        action, value  = agent(list(observation.values()))\n","        observation, reward, done, info = env.step(action,e,t)\n","        avg_reward+=reward\n","        print('e:',e,',t:',t,',action:',action,',state:',observation,',reward:',reward)\n","        agent.update(reward=reward, terminal=done or (t==1))\n","        if done or t==99:\n","            print(\"Episode {} finished after {} timesteps with reward {}\".format(e+1, t+1, avg_reward/(t+1)))\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMgIEP-AQOfl"},"source":["#rewards_stats\n","#df=pd.read_csv(env.stats_dir)  #'./outputs/out1/overall_stats.csv'\n","df=pd.read_csv('./outputs/out1/overall_stats.csv')\n","rewards=np.array(df[' Store Util%'])  #reward\n","overflows=np.array(df[' Step'])  #overflow_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEJMd-E3jwLd"},"source":["#histogram\n","x=np.array(rewards)\n","plt.hist(x, density=True, bins=30)  #`density=False` would make counts\n","plt.ylabel('Probability')\n","plt.xlabel('Rewards');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJqRT_ZhuTNR"},"source":["#base_reward,max_reward(+index)\n","max_rewards=max(rewards)\n","print('Maximum reward: %f'%(max_rewards))\n","print('Maximum reward index: ',np.argmax(rewards))\n","#print('Base reward: ',env.base_reward)\n","count=0\n","for i in range(len(rewards)):\n","    if rewards[i]>0:\n","        count+=1\n","#        print('hi',i)\n","print('count: ',count,count/len(rewards))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3I70OVwP7Lg"},"source":["#plots\n","plt.plot(rewards[0:400])\n","#plt.plot(overflows)\n","#plt.plot(overflows)\n","plt.xlabel('Steps')\n","plt.ylabel('Reward')\n","plt.title('Reward vs Steps for constant_p=100, constant_n=20, prop_neg=True')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZkTs80ejG2H"},"source":["#rough\n","max(df[' Store Util%'])\n","df[' Step']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSik6u5rLY0R"},"source":["#resent50\n","import torch\n","model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)\n","# or any of these variants\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet34', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet101', pretrained=True)\n","# model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet152', pretrained=True)\n","model.eval()"],"execution_count":null,"outputs":[]}]}